    PROJECT AIMS
 
    • The aim of this study is to find the possibilities of using Deep Neural Network (DNN) for translating sign languages into speech and text format through human       hand gestures.
 
    • The main objective of this study is to explore the feasibility of introducing a vision-based application,
    for the translation of the sign language to eradicate communication barriers between the hearing
    blind and deaf communities.
    • Sign language is translated into a text and speech format by recognizing hand signs.
    • The system has been developed following the training of a pre-trained SSD-Network using the captured dataset.
    • This system could be researched further and analyze the different types of sign language vocabulary and improve the interface aspect.

    PROJECT OBJECTIVES

     • To provide and establish pursuit gesture communication with deaf and blind people by converting hand gestures from sign language into digital text.
     • To evaluate the performance of the deep neural network and then import the data collected from human hand gestures into Convolutional Neural Network (CNN).
     • To achieve an overall training accuracy score of 90% or higher and more reliable performance metrics that can be used to validate and predict the output data of      sign language hand gesture.
     • To develop and test a trained neural network model to predict sign language hand gestures with a high level of accuracy for both the audience and the deaf            signer.  
     • To discuss the results and to make recommendations for the future improvements.

     ![Picture1](https://user-images.githubusercontent.com/58666940/168078886-63cb904d-e574-40b3-a7fb-43a99961a7af.png)
      Structure of the conceptual framework
